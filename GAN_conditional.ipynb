{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/Robin_Koester/Desktop/GANs/Energy_Data/DayAhead_hour_20152017.csv', newline = '') as csvfile:\n",
    "    datareader = csv.reader(csvfile, delimiter = ';')\n",
    "    \n",
    "    data_array = np.array(list(datareader))    \n",
    "    \n",
    "price_array = np.array([np.array(data_array[i]) for i in range(1,len(data_array))])\n",
    "\n",
    "prices = np.array(list(map(lambda it: float(price_array[it,1]) , range(1,len(price_array))) ))\n",
    "\n",
    "date_array = np.array(list(map(lambda it: datetime.strptime(price_array[it,0],'%d.%m.%Y %H:%M') , range(1,len(price_array))) ))\n",
    "\n",
    "day_of_week_array = np.array(list(map(lambda date: datetime.weekday(date), date_array)))\n",
    "\n",
    "hour_array = np.array(list(map(lambda date: datetime.time(date).hour, date_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; D_loss: 7.605; G_loss: 7.613\n",
      "2401.8862\n",
      "Iter: 200; D_loss: 7.224; G_loss: 7.721\n",
      "2406.5186\n",
      "Iter: 400; D_loss: 7.222; G_loss: 7.721\n",
      "2407.2744\n",
      "Iter: 600; D_loss: 7.222; G_loss: 7.721\n",
      "2408.0547\n",
      "Iter: 800; D_loss: 7.221; G_loss: 7.721\n",
      "2408.3008\n",
      "Iter: 1000; D_loss: 7.221; G_loss: 7.721\n",
      "2408.4016\n",
      "Iter: 1200; D_loss: 7.221; G_loss: 7.721\n",
      "2408.5723\n",
      "Iter: 1400; D_loss: 7.221; G_loss: 7.721\n",
      "2408.6694\n",
      "Iter: 1600; D_loss: 7.221; G_loss: 7.721\n",
      "2408.7441\n",
      "Iter: 1800; D_loss: 7.221; G_loss: 7.721\n",
      "2408.7769\n",
      "Iter: 2000; D_loss: 7.222; G_loss: 7.721\n",
      "2408.838\n",
      "Iter: 2200; D_loss: 7.221; G_loss: 7.721\n",
      "2408.8718\n",
      "Iter: 2400; D_loss: 7.221; G_loss: 7.721\n",
      "2408.899\n",
      "Iter: 2600; D_loss: 7.221; G_loss: 7.721\n",
      "2408.9177\n",
      "Iter: 2800; D_loss: 7.222; G_loss: 7.721\n",
      "2408.938\n",
      "Iter: 3000; D_loss: 7.222; G_loss: 7.721\n",
      "2408.9534\n",
      "Iter: 3200; D_loss: 7.221; G_loss: 7.721\n",
      "2408.9673\n",
      "Iter: 3400; D_loss: 7.221; G_loss: 7.721\n",
      "2408.9968\n",
      "Iter: 3600; D_loss: 7.221; G_loss: 7.721\n",
      "2409.0105\n",
      "Iter: 3800; D_loss: 7.221; G_loss: 7.721\n",
      "2409.0361\n",
      "Iter: 4000; D_loss: 7.221; G_loss: 7.721\n",
      "2409.0903\n",
      "Iter: 4200; D_loss: 7.221; G_loss: 7.721\n",
      "2409.1204\n",
      "Iter: 4400; D_loss: 7.221; G_loss: 7.721\n",
      "2409.1482\n",
      "Iter: 4600; D_loss: 7.221; G_loss: 7.721\n",
      "2409.1694\n",
      "Iter: 4800; D_loss: 7.221; G_loss: 7.721\n",
      "2409.1938\n",
      "Iter: 5000; D_loss: 7.221; G_loss: 7.721\n",
      "2409.2097\n",
      "Iter: 5200; D_loss: 7.221; G_loss: 7.721\n",
      "2409.225\n",
      "Iter: 5400; D_loss: 7.221; G_loss: 7.721\n",
      "2409.2373\n",
      "Iter: 5600; D_loss: 7.222; G_loss: 7.721\n",
      "2409.2468\n",
      "Iter: 5800; D_loss: 7.221; G_loss: 7.721\n",
      "2409.263\n",
      "Iter: 6000; D_loss: 7.221; G_loss: 7.721\n",
      "2409.2913\n",
      "Iter: 6200; D_loss: 7.221; G_loss: 7.721\n",
      "2409.3003\n",
      "Iter: 6400; D_loss: 7.221; G_loss: 7.721\n",
      "2409.3135\n",
      "Iter: 6600; D_loss: 7.221; G_loss: 7.721\n",
      "2409.3237\n",
      "Iter: 6800; D_loss: 7.221; G_loss: 7.721\n",
      "2409.3333\n",
      "Iter: 7000; D_loss: 7.222; G_loss: 7.721\n",
      "2409.3408\n",
      "Iter: 7200; D_loss: 7.221; G_loss: 7.721\n",
      "2409.3462\n",
      "Iter: 7400; D_loss: 7.221; G_loss: 7.721\n",
      "2409.3457\n",
      "Iter: 7600; D_loss: 7.221; G_loss: 7.721\n",
      "2409.3516\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-af99ad385c19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0mX_mb_day1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_mb_day2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweekday\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhour\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     \u001b[0mz_mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_z\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-194-af99ad385c19>\u001b[0m in \u001b[0;36mnext_batch\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mind_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mX_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mind_vec_shuffled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind_vec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mmb_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mind_vec_shuffled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\env1\\lib\\random.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, population, k)\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m         \u001b[1;31m# invariant:  non-selected at [0,n-i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                 \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                 \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m                 \u001b[0mpool\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;31m# move non-selected item into vacancy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# hyperparameters \n",
    "\n",
    "mb_size = 1000                    # size of mini batches, can be randomly sampled\n",
    "time_predict_dim = 24\n",
    "time_conditional_dim = 24       # the whole snippet of the time series should comprise two day\n",
    "z_dim = 0                      # number of random inputs\n",
    "h_dim = 128                     # number of neurons in the hidden layer: ohne hidden layer in the each network\n",
    "lr = 1e-3                       # learning rate\n",
    "d_steps = 3\n",
    "no_iterations = 50000                # number of steps for the training algorithm\n",
    "reg_scale = 1e-2\n",
    "\n",
    "gen_input_dim = z_dim + time_predict_dim + 2           # conditional inputs and z_dim random inputs  (input of the generator net)\n",
    "discr_input_dim = time_predict_dim + time_conditional_dim + 2\n",
    "\n",
    "# definition of functions\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)   # return the respective initialized weights\n",
    "\n",
    "\n",
    "def log(x):\n",
    "    return tf.log(x + 1e-8)\n",
    "\n",
    "\n",
    "def sample_z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "def next_batch(size):\n",
    "    ind_vec = list(range(len(prices)-X_dim))\n",
    "    ind_vec_shuffled = random.sample(ind_vec,len(ind_vec))\n",
    "\n",
    "    mb_ind = ind_vec_shuffled[0:(size)]\n",
    "    \n",
    "    mb_prices_day1 = np.array(list(map(lambda mb_first_ind: prices[mb_first_ind:(mb_first_ind+(X_dim-24))], mb_ind)))   \n",
    "    mb_prices_day2 = np.array(list(map(lambda mb_first_ind: prices[(mb_first_ind+(X_dim-24)):(mb_first_ind+X_dim)], mb_ind)))\n",
    "    \n",
    "    weekday = np.array(list(map(lambda mb_first_ind: day_of_week_array[mb_first_ind+(X_dim-24)], mb_ind))).reshape(size,1)\n",
    "    hour = np.array(list(map(lambda mb_first_ind: hour_array[mb_first_ind+(X_dim-24)], mb_ind))).reshape(size,1)\n",
    "    \n",
    "    return mb_prices_day1, mb_prices_day2, weekday, hour\n",
    "\n",
    "\n",
    "# generative network\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, discr_input_dim])\n",
    "conditional = tf.placeholder(tf.float32, shape = [None, time_conditional_dim + 2])\n",
    "z = tf.placeholder(tf.float32, shape=[None, gen_input_dim])\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([gen_input_dim, h_dim]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "G_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "G_W3 = tf.Variable(xavier_init([h_dim, time_predict_dim]))\n",
    "G_b3 = tf.Variable(tf.zeros(shape=[time_predict_dim]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
    "\n",
    "\n",
    "# discriminative network\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([discr_input_dim, h_dim]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "D_W2 = tf.Variable(xavier_init([h_dim, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "\n",
    "# functions defining operations in networks\n",
    "def G(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)    # bekommt tensor übergeben (mbsize, z_inp_dim)\n",
    "    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)    # bekommt tensor übergeben (mbsize, z_inp_dim)\n",
    "    G_h3 = tf.nn.relu(tf.matmul(G_h2, G_W3) + G_b3)             # output layer hat die dimension (mbsize, X_dim)\n",
    "    G_output = tf.sigmoid(G_h3)\n",
    "    return G_h3, G_output\n",
    "\n",
    "\n",
    "def D(X):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(X, D_W1) + D_b1)    # bekommt tensor übergeben (mbsize, discr_inp_dim)\n",
    "    out = tf.sigmoid(tf.matmul(D_h1, D_W2) + D_b2)  # output hat die dimension (mbsize, 1)\n",
    "    return out\n",
    "\n",
    "G_sample_wo_scaling, G_sample = G(z)       #  hat die dimension (mbsize,t_predict_dim)\n",
    "\n",
    "D_real = D(X)         #  X hat die dimension (mbsize, t_predict_dim + t_conditional_dim + 2)\n",
    "\n",
    "G_sample_cat = tf.concat([G_sample, conditional],1)\n",
    "\n",
    "D_fake = D(G_sample_cat)  #  \n",
    "\n",
    "D_target = 1./mb_size        #  this is the target of the discriminator net: exactly the number of real price series\n",
    "G_target = 1./(mb_size*2)    #  this is the target of the gen net (for every time series 1/2)\n",
    "\n",
    "Z = tf.reduce_sum(tf.exp(-D_real)) + tf.reduce_sum(tf.exp(-D_fake))\n",
    "\n",
    "# the minimax game\n",
    "D_loss = tf.reduce_sum(D_target * D_real) + log(Z)\n",
    "G_loss = tf.reduce_sum(G_target * D_real) + tf.reduce_sum(G_target * D_fake) + log(Z)\n",
    "\n",
    "reg_loss = tf.reduce_sum(tf.abs(G_W1)) + tf.reduce_sum(tf.abs(G_W2))\n",
    "\n",
    "G_loss_total = tf.add(G_loss, reg_scale * reg_loss, name = \"total_gen_loss\")\n",
    "\n",
    "\n",
    "# the training operation\n",
    "D_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(D_loss, var_list=theta_D))\n",
    "G_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(G_loss, var_list=theta_G))\n",
    "\n",
    "\n",
    "# initialize the place holder nodes\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(no_iterations):\n",
    "    \n",
    "    X_mb_day1, X_mb_day2, weekday, hour = next_batch(mb_size)\n",
    "    z_mb = sample_z(mb_size, z_dim)\n",
    "    \n",
    "    \n",
    "    inputs_gen = np.concatenate((z_mb, X_mb_day1, weekday, hour), axis = 1)\n",
    "    inputs_discr = np.concatenate((X_mb_day2, X_mb_day1, weekday, hour), axis = 1)\n",
    "    conditional_array = np.concatenate((X_mb_day1, weekday, hour),axis = 1)\n",
    "    \n",
    "    _, D_loss_curr = sess.run(\n",
    "        [D_solver, D_loss], feed_dict={X: inputs_discr, z: inputs_gen, conditional: conditional_array}\n",
    "    )\n",
    "\n",
    "    _, G_loss_curr, regularization_loss = sess.run(\n",
    "        [G_solver, G_loss, reg_loss], feed_dict={X: inputs_discr, z: inputs_gen, conditional: conditional_array}\n",
    "    )\n",
    "\n",
    "    if it % 200 == 0:\n",
    "        print('Iter: {}; D_loss: {:.4}; G_loss: {:.4}'\n",
    "              .format(it, D_loss_curr, G_loss_curr))\n",
    "        print(regularization_loss)\n",
    "        \n",
    "        \n",
    "        X_mb_day1, X_mb_day2, weekday, hour = next_batch(1)\n",
    "        z_mb = sample_z(1, z_dim)\n",
    "        \n",
    "        inputs_gen = np.concatenate((z_mb, X_mb_day1, weekday, hour), axis = 1)\n",
    "        inputs_discr = np.concatenate((X_mb_day2, X_mb_day1, weekday, hour), axis = 1)\n",
    "        conditional_array = np.concatenate((X_mb_day1, weekday, hour),axis = 1)\n",
    "\n",
    "        samples = sess.run(G_sample_wo_scaling, feed_dict={z: inputs_gen})\n",
    "        \n",
    "        samples_compl = np.concatenate((X_mb_day1, samples), axis = 1)\n",
    "        \n",
    "        real_data = np.concatenate((X_mb_day1, X_mb_day2), axis = 1)\n",
    "\n",
    "       # print(np.ndarray.flatten(samples))\n",
    "        fig = plt.plot(np.ndarray.flatten(samples_compl))\n",
    "        fig = plt.plot(np.ndarray.flatten(real_data))\n",
    "        plt.savefig('out/{}.png'\n",
    "                    .format(str(i).zfill(4)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        fig = plt.gcf()\n",
    "        plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing !!!\n",
    "\n",
    "X_dim = 48 \n",
    "\n",
    "def next_batch(size):\n",
    "    ind_vec = list(range(len(prices)-X_dim))\n",
    "    ind_vec_shuffled = random.sample(ind_vec,len(ind_vec))\n",
    "\n",
    "    mb_ind = ind_vec_shuffled[0:(size)]\n",
    "    \n",
    "    mb_prices_day1 = np.array(list(map(lambda mb_first_ind: prices[mb_first_ind:(mb_first_ind+(X_dim-24))], mb_ind)))\n",
    "    \n",
    "    mb_prices_day2 = np.array(list(map(lambda mb_first_ind: prices[(mb_first_ind+(X_dim-24)+1):(mb_first_ind+X_dim)], mb_ind)))\n",
    "    \n",
    "    return mb_prices_day1, mb_prices_day2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1: \n",
      "\n",
      " [[23.08 23.06 23.06 25.61 28.65 31.99 34.05 38.7  38.98 34.08 30.65 26.76\n",
      "  24.05 22.87 22.51 21.73 21.1  22.09 22.94 22.93 24.07 25.11 24.96 25.48]] \n",
      "\n",
      "\n",
      "Day 2: \n",
      "\n",
      " [[25.16 24.54 23.48 23.42 24.01 28.73 36.36 39.85 42.57 38.93 35.94 31.27\n",
      "  24.06 23.45 22.88 22.15 22.75 24.12 36.69 43.   42.53 39.43 33.5  30.71]] \n",
      "\n",
      "\n",
      "(1, 24)\n",
      "(1, 24)\n",
      "[[5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6]]\n",
      "[[6 6 6 6 6 6 6 6 6 6 6 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def next_batch(size):\n",
    "    ind_vec = list(range(len(prices)-X_dim))\n",
    "    ind_vec_shuffled = random.sample(ind_vec,len(ind_vec))\n",
    "\n",
    "    mb_ind = ind_vec_shuffled[0:(size)]\n",
    "    \n",
    "    mb_prices_day1 = np.array(list(map(lambda mb_first_ind: prices[mb_first_ind:(mb_first_ind+(X_dim-24))], mb_ind)))   \n",
    "    mb_prices_day2 = np.array(list(map(lambda mb_first_ind: prices[(mb_first_ind+(X_dim-24)):(mb_first_ind+X_dim)], mb_ind)))\n",
    "    \n",
    "    days_of_the_week_day1 = np.array(list(map(lambda mb_first_ind: day_of_week_array[mb_first_ind:(mb_first_ind+(X_dim-24))], mb_ind))) \n",
    "    days_of_the_week_day2 = np.array(list(map(lambda mb_first_ind: day_of_week_array[(mb_first_ind+(X_dim-24)):(mb_first_ind+X_dim)], mb_ind)))\n",
    "    \n",
    "    return mb_prices_day1, mb_prices_day2, days_of_the_week_day1, days_of_the_week_day2\n",
    "\n",
    "\n",
    "a, b, c, d = next_batch(1)\n",
    "print('Day 1: \\n\\n', a,'\\n\\n')\n",
    "print('Day 2: \\n\\n',b,'\\n\\n')\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 40)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "c = sample_z(2,40)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A =np.concatenate((a,c),axis = 1)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
